---
layout: post-svm-vis
title:  "Visualizing Convergence Algorithms"
date:   2017-04-22 08:35:48 -0500
categories: project
---

**What is this?**

To gain an intuitive understanding of how gradient descent methods converge to a solution, we created a visualization of an SVM learning to seperate 2 dimensional data.

You can select the solver algorithm, the hyper-parameters of the solvers. You may find that decreasing the learning rate is necessary for the solver to give a stable solution. Our data is randomly generated using a guassian distribution centered at an equal distance from the origin. You can change the mean of the distribution to see how solvers handle data which is and is not seperable.

A description of each solver is below the visualization. 

The number labeled `i` in the corner of the visualization is the iteration over the data set: for each example processed, i is incremented.

The number labeled `e` is the epoch, or how many times the complete data set has been parsed over.

