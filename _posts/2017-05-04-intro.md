---
layout: page
title:  "Introduction"
date:   2017-05-04 08:35:48 -0500
categories: project
---

# Understanding Variance Reduction Techniques with SGD

Stochastic Gradient Descent (SGD) is the workhorse behind many machine learning tasks. It's used to train many common classifiers like linear classifiers and SVMs, as well as large non-convex problems like neural networks. In general, SGD has a small memory footprint, learns quickly, and is robust to noise- all good things to have in a training algorithm. However, SGD has high variance between applications of the gradient function which can be inefficient. The research community has addressed this problem by introducing Variance Reduction (VR) techniques.

In this article, we wish to give an intuitive understanding of how variance reduction (VR) techniques work when applied to SGD. To this end, we have constructed an interactive visualization of an SVM learning in real time, using one of 3 possible update algorithms. We also implemented several algorithms in C++ and tested them on real world datasets to learn how the algorithm's differ in time to convergence.

## SGD Background

In its simplest form, we can write the stochastic update function as

$$w_{i+1} = w_{i} - \eta(\nabla f_i(x_j))$$

where $$x_j$$ is single randomly selected training example, and $$w_i$$ is the weight vector at time $$i$$. $$\nabla f$$ is the gradient of our loss function, $$f$$. Each iteration $$i$$ changes the weight vector by taking a step of size $$\eta$$ in the opposite direction of greatest positive change (i.e. we are reducing the loss). After some number of iterations, or when $$w_{i} - w_{i+1}$$ is sufficiently small, we consider ourselves converged.

SGD is often compared to the Gradient Descent which, instead of calculating $$\nabla f_i(x_j)$$, calculates $$\nabla f_i(X)$$ where $$X$$ is the entire training set. $$\nabla f_i(X)$$ is sometimes called the *true gradient* because it is the actual gradient at $$i$$ whereas $$\nabla f_i(x_j)$$ is merely an approximation of the true gradient at $$i$$. Because gradients in GD are accurate, we can use a larger learning rate than SGD. However, SGD generally converges faster than GD. If $$n$$ is the number of training examples, we generally use let $$\eta _{SGD} > \frac{\eta _{GD}}{n}$$ meaning that if SGD's gradients are mostly accurate, we should converge faster than GD because we apply many more gradient per unit time.

One of the main weaknesses of SGD is the imprecision of the stochastic gradients. The problem is that gradients tend to bounce around in varying directions, so instead of smoothly approaching our converged error, we will tend to jitter. 

<a href="/assets/SGDvGD.svg"><img src="/assets/SGDvGD.svg"></a>

The image shows a sketch of the error rate as we train. In the ideal case, error should be strictly decreasing, however, this is often not the case for SGD.

## VR Algorithms Overview

**[TODO]** Give an intuition for what VR is doing (Variance, expected values, etc).

### ASGD

Averaged Stochastic Gradient Descent is the simplest method of variance reduction. We use the formulation given by Leon Bottou in {% cite bottou-2012-tricks %}. The VR stochastic update works by keeping an average of some set of the weight vectors, and using this average as the true weight vector. We'll define the average as

$$\bar{w}_t = \frac{1}{t - t_0}\sum_{i=t_0}^{t}w_i$$

This means that we will need to select a $$t_0$$. There's no clear guideline on how to do this, so in our experiments and in our visualization, we choose to begin averaging after the first epoch. For each update before $$t_0$$, we do a normal stochastic update:

$$w_{t+1} = w_{t} - \eta(\nabla f_t(x_j))$$

Then, we swap out the stochastic weight vector with an averaged one, given as:

$$\bar{w}_{t+1} = \bar{w_t} + \mu_t (\bar{w_t} - w_t)$$

where $$\mu_t$$ is $$\frac{1}{max(t-t_0, 1)}$$. The max is needed in case we start $$t_0$$ at time zero.

Notice, we still compute the stochastic gradient at each iteration, but we keep a running average the gradient since $$t$$.

**[TODO]** Could use comments on this paragraph: It's easy to see how variance is reduced using this method, as now we take only small differences from the average and the newly computed gradient. That, and since we choose time $$t_0$$ to be large, we should already be close to converging, so changes will be minimal anyways.

### SAG

SAG, Stochastic Averaged Gradient, builds on the ASGD method by keeping a history of $$n$$ past gradients, as well as the moving average of the last epoch's worth of updates. The method is described thoroughly in {% cite schmidt-2013-sag %}.

The update function is given as:

$$w_{t+1} = w_{t} - \eta \BigL $$

We see that now we using a moving average over the 


### Works Cited

{% bibliography --cited --file references %}
